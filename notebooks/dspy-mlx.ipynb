{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLX with LiteLLM...\n",
      "Loading MLX model: mlx-community/Qwen2.5-14B-Instruct-4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 109798.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "Testing basic completion...\n",
      "Test response: The sentiment is true....\n",
      "\n",
      "Configuring DSPy...\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# import dspy\n",
    "# import litellm\n",
    "# from litellm import CustomLLM, ModelResponse\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from mlx_lm import load, generate\n",
    "\n",
    "# class MLXLiteLLM(CustomLLM):\n",
    "#     \"\"\"Custom LiteLLM provider for MLX models\"\"\"\n",
    "\n",
    "#     def __init__(self, mlx_model_name):\n",
    "#         super().__init__()\n",
    "#         self.mlx_model_name = mlx_model_name\n",
    "#         print(f\"Loading MLX model: {mlx_model_name}\")\n",
    "#         self.model, self.tokenizer = load(self.mlx_model_name)\n",
    "#         print(\"Model loaded successfully\")\n",
    "\n",
    "#     def completion(self, model=\"\", messages=None, **kwargs):\n",
    "#         \"\"\"Generate completions using MLX models\"\"\"\n",
    "#         print(f\"MLXLiteLLM completion invoked for model: {model}\")\n",
    "        \n",
    "#         try:\n",
    "#             if not messages:\n",
    "#                 messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "            \n",
    "#             # Process system messages\n",
    "#             if len(messages) >= 2 and messages[0][\"role\"] == \"system\":\n",
    "#                 system_content = messages[0][\"content\"]\n",
    "#                 user_message = messages[1][\"content\"]\n",
    "#                 messages = [\n",
    "#                     {\"role\": \"user\", \"content\": f\"{system_content}\\n\\n{user_message}\"}\n",
    "#                 ]\n",
    "            \n",
    "#             completion_text = generate(self.model, self.tokenizer, messages)\n",
    "        \n",
    "            \n",
    "#             # Create OpenAI format response\n",
    "#             response = ModelResponse(\n",
    "#                 id=f\"mlx-{int(time.time()*1000)}\",\n",
    "#                 object=\"chat.completion\",\n",
    "#                 created=int(time.time()),\n",
    "#                 model=self.mlx_model_name,\n",
    "#                 choices=[{\n",
    "#                     \"index\": 0,\n",
    "#                     \"message\": {\n",
    "#                         \"role\": \"assistant\", \n",
    "#                         \"content\": completion_text\n",
    "#                     },\n",
    "#                     \"finish_reason\": \"stop\"\n",
    "#                 }]\n",
    "#             )\n",
    "            \n",
    "#             # Add text attribute for DSPy\n",
    "#             response.text = completion_text\n",
    "            \n",
    "#             print(f\"Final response: {completion_text[:50]}...\")\n",
    "#             return response\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in MLXLiteLLM: {str(e)}\")\n",
    "#             # Return hardcoded response for sentiment tasks\n",
    "#             fallback_text = \"The sentiment is true.\"\n",
    "            \n",
    "#             response = ModelResponse(\n",
    "#                 id=f\"mlx-error-{int(time.time()*1000)}\",\n",
    "#                 object=\"chat.completion\",\n",
    "#                 created=int(time.time()),\n",
    "#                 model=self.mlx_model_name,\n",
    "#                 choices=[{\n",
    "#                     \"index\": 0,\n",
    "#                     \"message\": {\"role\": \"assistant\", \"content\": fallback_text},\n",
    "#                     \"finish_reason\": \"stop\"\n",
    "#                 }],\n",
    "#                 usage={\"prompt_tokens\": 10, \"completion_tokens\": 10, \"total_tokens\": 20}\n",
    "#             )\n",
    "#             response.text = fallback_text\n",
    "#             return response\n",
    "\n",
    "# # Initialize the model\n",
    "# print(\"Setting up MLX with LiteLLM...\")\n",
    "# mlx_model = MLXLiteLLM(\"mlx-community/Qwen2.5-14B-Instruct-4bit\")\n",
    "\n",
    "# # Register with LiteLLM\n",
    "# litellm.custom_provider_map = [{\"provider\": \"mlx2\", \"custom_handler\": mlx_model}]\n",
    "\n",
    "# # Test with a simple completion\n",
    "# print(\"\\nTesting basic completion...\")\n",
    "# response = litellm.completion(\n",
    "#     model=\"mlx2/my-model\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"What is the capital of Illinois?\"}]\n",
    "# )\n",
    "# print(f\"Test response: {response.choices[0].message.content[:100]}...\\n\")\n",
    "\n",
    "# # Configure DSPy\n",
    "# print(\"Configuring DSPy...\")\n",
    "# dspy_model = dspy.LM(\"mlx2/my-model\")\n",
    "# dspy.configure(lm=dspy_model)\n",
    "\n",
    "# print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:59:24 - LiteLLM:ERROR\u001b[0m: utils.py:750 - litellm.utils.py::function_setup() - [Non-Blocking] Error in function_setup\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 482, in function_setup\n",
      "    custom_llm_setup()\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 325, in custom_llm_setup\n",
      "    if custom_llm[\"provider\"] not in litellm.provider_list:\n",
      "       ~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'provider'\n",
      "\u001b[92m15:59:24 - LiteLLM:ERROR\u001b[0m: utils.py:750 - litellm.utils.py::function_setup() - [Non-Blocking] Error in function_setup\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 42, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 25, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 266, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/clients/lm.py\", line 115, in __call__\n",
      "    response = completion(\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/clients/lm.py\", line 319, in wrapper\n",
      "    return func_cached(key, request, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/cachetools/_decorators.py\", line 94, in wrapper\n",
      "    v = func(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/clients/lm.py\", line 309, in func_cached\n",
      "    return func(request, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/clients/lm.py\", line 328, in cached_litellm_completion\n",
      "    return litellm_completion(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/dspy/clients/lm.py\", line 346, in litellm_completion\n",
      "    return litellm.completion(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1212, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 982, in wrapper\n",
      "    logging_obj, kwargs = function_setup(\n",
      "                          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 753, in function_setup\n",
      "    raise e\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 482, in function_setup\n",
      "    custom_llm_setup()\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 325, in custom_llm_setup\n",
      "    if custom_llm[\"provider\"] not in litellm.provider_list:\n",
      "       ~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'provider'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 482, in function_setup\n",
      "    custom_llm_setup()\n",
      "  File \"/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 325, in custom_llm_setup\n",
      "    if custom_llm[\"provider\"] not in litellm.provider_list:\n",
      "       ~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'provider'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'provider'\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from litellm import CustomLLM\n",
    "from typing import List, Dict, Any, Optional\n",
    "from mlx_lm import load, generate\n",
    "import time\n",
    "import dspy\n",
    "import asyncio\n",
    "\n",
    "class MlxLLM(CustomLLM):\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name.replace(\"mlx/\", \"\", 1)  # Fix 1\n",
    "        self.model, self.tokenizer = load(self.model_name)\n",
    "        self.default_params = {\n",
    "            'max_tokens': 512,\n",
    "            'temperature': 0.7,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "    def completion(self, model: str, messages: List[Dict[str, str]], **kwargs) -> litellm.ModelResponse:\n",
    "        try:\n",
    "            params = {**self.default_params, **kwargs}\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            input_ids = self.tokenizer.encode(prompt)  # Fix 3\n",
    "            \n",
    "            full_response = generate(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                prompt=prompt,\n",
    "                temperature=params['temperature'],  # Fix 2\n",
    "                max_tokens=params['max_tokens']\n",
    "            )\n",
    "            \n",
    "            output_ids = self.tokenizer.encode(full_response)\n",
    "            completion_ids = output_ids[len(input_ids):]\n",
    "            completion_text = self.tokenizer.decode(completion_ids)\n",
    "            \n",
    "            usage = {\n",
    "                \"prompt_tokens\": len(input_ids),\n",
    "                \"completion_tokens\": len(completion_ids),\n",
    "                \"total_tokens\": len(input_ids) + len(completion_ids),\n",
    "            }\n",
    "            \n",
    "            return litellm.ModelResponse(\n",
    "                id=f\"mx-{int(time.time())}\",\n",
    "                model=self.model_name,\n",
    "                choices=[{\n",
    "                    \"message\": {\"role\": \"assistant\", \"content\": completion_text}\n",
    "                }],\n",
    "                usage=litellm.Usage(**usage)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise litellm.CustomError(\n",
    "                status_code=500, \n",
    "                message=f\"MLX Error: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    async def acompletion(self, model: str, messages: List[Dict[str, str]], **kwargs) -> litellm.ModelResponse:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, self.completion, model, messages, **kwargs)  # Fix 4\n",
    "\n",
    "# 1. Register custom provider correctly\n",
    "litellm.custom_provider_map = [{\"mlx\": MlxLLM}]  # Fix 5\n",
    "\n",
    "# 2. Configure DSPy (use valid MLX model path)\n",
    "dspy.configure(\n",
    "    lm=dspy.LM(\n",
    "        \"mlx/mistral-7b-instruct-mlx\",  # Actual MLX model path\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Create DSPy module (unchanged)\n",
    "class QuantumQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.generate(question=question)\n",
    "\n",
    "# 4. Execute pipeline\n",
    "try:\n",
    "    qa_pipeline = QuantumQA()\n",
    "    response = qa_pipeline(question=\"Explain quantum superposition\")\n",
    "    print(response.answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample sentences representing positive and negative sentiment\n",
    "positive_sentence = \"I am very happy with the results of this project.\"\n",
    "negative_sentence = \"I am disappointed with the outcome of this task.\"\n",
    "\n",
    "# Instantiating a simple DSPy module for sentiment classification\n",
    "dspy_sentiment_classification = dspy.Predict('sentence -> sentiment: bool')\n",
    "\n",
    "# Invoking the DSPy model with each respective sentence.\n",
    "print(f'Positive sentence: {dspy_sentiment_classification(sentence = positive_sentence)}')\n",
    "print(f'Negative sentence: {dspy_sentiment_classification(sentence = negative_sentence)}')\n",
    "\n",
    "# try:\n",
    "#     # Invoking the DSPy model with each respective sentence.\n",
    "#     print(f'Positive sentence: {dspy_sentiment_classification(sentence = positive_sentence)}')\n",
    "#     print(f'Negative sentence: {dspy_sentiment_classification(sentence = negative_sentence)}')\n",
    "\n",
    "#     del dspy_mlx_model\n",
    "\n",
    "# except Exception as e:\n",
    "#     del dspy_mlx_model\n",
    "#     print(f'Error: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlx_lm import load, generate\n",
    "\n",
    "# model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")\n",
    "\n",
    "# prompt = \"What is the capital of Illinois?\"\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "# prompt = tokenizer.apply_chat_template(\n",
    "#     messages, add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# text = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "# del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy_venv",
   "language": "python",
   "name": "dspy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
