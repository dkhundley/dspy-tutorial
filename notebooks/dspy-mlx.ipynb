{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'id': 'chatcmpl-f93cc5d5-9cbf-4142-8330-fd5f174a6517', 'system_fingerprint': '0.21.5-0.23.2-macOS-15.3.1-arm64-arm-64bit-applegpu_g16g', 'object': 'chat.completion', 'model': 'default_model', 'created': 1741972254, 'choices': [{'index': 0, 'logprobs': {'token_logprobs': [-0.234375, 0.0, -0.328125, -0.859375, -0.09375, -0.75, 0.0, 0.0, -0.0625, -0.421875, -0.25, 0.0, -0.015625, 0.0, -0.65625, -1.84375, -0.09375, -0.703125, -0.21875, -0.0625, -0.53125, -2.3125, -0.265625, -0.25, 0.0, -4.1328125, 0.0, 0.0, 0.0, -0.25, -0.0625], 'top_logprobs': [], 'tokens': [7373, 8999, 29576, 1619, 1117, 8934, 1032, 2137, 29491, 2370, 1309, 1083, 6799, 1136, 1163, 1342, 2137, 1210, 2084, 1136, 3148, 1032, 6703, 2641, 29572, 3962, 2114, 1296, 1641, 29576, 2]}, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': 'Understood! This is indeed a test. How can I assist you with your test or help you understand a concept better? Just let me know!'}}], 'usage': {'prompt_tokens': 11, 'completion_tokens': 31, 'total_tokens': 42}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import requests\n",
    "\n",
    "with open('../keys/ip_addresses.yaml', 'r') as f:\n",
    "    hundley_mac_mini_ip = yaml.safe_load(f)['HUNDLEY_MAC_MINI_LOCAL_IP']\n",
    "\n",
    "# API endpoint\n",
    "url = f\"http://{hundley_mac_mini_ip}:8585/v1/chat/completions\"\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Print the response\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dkhundley/Documents/Repositories/dspy-tutorial/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentence: Prediction(\n",
      "    sentiment=True\n",
      ")\n",
      "Negative sentence: Prediction(\n",
      "    sentiment=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import litellm\n",
    "from litellm import CustomLLM\n",
    "import dspy\n",
    "import uuid\n",
    "from time import time\n",
    "\n",
    "class MyCustomAPILLM(CustomLLM):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.url = f\"http://{hundley_mac_mini_ip}:8585/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "    def _process_messages(self, messages):\n",
    "        \"\"\"Process messages to ensure they're in the correct format\"\"\"\n",
    "        processed_messages = []\n",
    "        \n",
    "        # Extract any system messages\n",
    "        system_content = \"\"\n",
    "        for msg in messages:\n",
    "            # Convert message objects to dicts if needed\n",
    "            if not isinstance(msg, dict):\n",
    "                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n",
    "                    msg = {\"role\": msg.role, \"content\": msg.content}\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # Handle system messages\n",
    "            if msg.get(\"role\") == \"system\":\n",
    "                system_content += msg.get(\"content\", \"\") + \"\\n\"\n",
    "                continue\n",
    "                \n",
    "            # Keep user and assistant messages\n",
    "            if msg.get(\"role\") in [\"user\", \"assistant\"]:\n",
    "                processed_messages.append({\n",
    "                    \"role\": msg[\"role\"],\n",
    "                    \"content\": msg[\"content\"]\n",
    "                })\n",
    "        \n",
    "        # Ensure we start with a user message\n",
    "        if not processed_messages or processed_messages[0][\"role\"] != \"user\":\n",
    "            processed_messages.insert(0, {\"role\": \"user\", \"content\": \"Hello\"})\n",
    "        \n",
    "        # Add system content to first user message if exists\n",
    "        if system_content and processed_messages:\n",
    "            for i, msg in enumerate(processed_messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    processed_messages[i][\"content\"] = system_content.strip() + \"\\n\\n\" + msg[\"content\"]\n",
    "                    break\n",
    "        \n",
    "        # Ensure alternating pattern\n",
    "        filtered_messages = []\n",
    "        expected_role = \"user\"\n",
    "        for msg in processed_messages:\n",
    "            if msg[\"role\"] == expected_role:\n",
    "                filtered_messages.append(msg)\n",
    "                expected_role = \"assistant\" if expected_role == \"user\" else \"user\"\n",
    "        \n",
    "        return filtered_messages\n",
    "        \n",
    "    def _format_openai_response(self, raw_response):\n",
    "        \"\"\"Convert the API response to OpenAI format\"\"\"\n",
    "        try:\n",
    "            # Create a properly formatted response\n",
    "            formatted_response = {\n",
    "                \"id\": str(uuid.uuid4()),  # Generate a unique ID string\n",
    "                \"object\": \"chat.completion\",\n",
    "                \"created\": int(time()),\n",
    "                \"model\": \"custom-model\",\n",
    "                \"choices\": [\n",
    "                    {\n",
    "                        \"index\": 0,\n",
    "                        \"message\": {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": raw_response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                        },\n",
    "                        \"finish_reason\": \"stop\"\n",
    "                    }\n",
    "                ],\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": 0,\n",
    "                    \"completion_tokens\": 0,\n",
    "                    \"total_tokens\": 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Try to get usage from the response if available\n",
    "            if \"usage\" in raw_response:\n",
    "                formatted_response[\"usage\"] = raw_response[\"usage\"]\n",
    "                \n",
    "            return formatted_response\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error formatting response: {str(e)}\")\n",
    "            \n",
    "    def completion(self, model, messages, temperature=0.7, max_tokens=None, **kwargs):\n",
    "        \"\"\"Sends completion request to custom API endpoint\"\"\"\n",
    "        try:\n",
    "            # Only include keys that are expected by your API\n",
    "            ignored_keys = [\"model_response\", \"print_verbose\", \"acompletion\", \"logging_obj\", \n",
    "                           \"optional_params\", \"litellm_params\", \"logger_fn\", \"custom_prompt_dict\", \n",
    "                           \"client\", \"encoding\"]\n",
    "            \n",
    "            # Process messages to ensure proper format\n",
    "            processed_messages = self._process_messages(messages)\n",
    "            \n",
    "            # Build clean payload\n",
    "            payload = {\n",
    "                \"messages\": processed_messages,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            \n",
    "            # Add max_tokens if provided\n",
    "            if max_tokens is not None:\n",
    "                payload[\"max_tokens\"] = max_tokens\n",
    "            \n",
    "            # Add basic parameters that your API might support\n",
    "            for key, value in kwargs.items():\n",
    "                if key not in ignored_keys and not callable(value):\n",
    "                    payload[key] = value\n",
    "            \n",
    "            # Make the API call\n",
    "            response = requests.post(\n",
    "                self.url, \n",
    "                headers=self.headers, \n",
    "                json=payload  # Using json parameter handles serialization for you\n",
    "            )\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Get raw response data\n",
    "                raw_response = response.json()\n",
    "                \n",
    "                # Format the response to match OpenAI's structure\n",
    "                formatted_response = self._format_openai_response(raw_response)\n",
    "                \n",
    "                # Create a ModelResponse using the formatted response\n",
    "                return litellm.ModelResponse(**formatted_response)\n",
    "            else:\n",
    "                # Handle API errors\n",
    "                raise Exception(f\"API request failed with status code: {response.status_code}, details: {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Provide detailed error information\n",
    "            raise Exception(f\"Error in custom LLM handler: {str(e)}\")\n",
    "\n",
    "# Create an instance of your custom LLM\n",
    "my_custom_llm = MyCustomAPILLM()\n",
    "\n",
    "# Register your custom provider with LiteLLM\n",
    "litellm.custom_provider_map = [\n",
    "    {\"provider\": \"my-custom-api\", \"custom_handler\": my_custom_llm}\n",
    "]\n",
    "\n",
    "# Set up DSPy to use your custom LLM\n",
    "llm = dspy.LM(\n",
    "    model=\"my-custom-api/custom-model\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "dspy.configure(lm = llm)\n",
    "\n",
    "# Creating sample sentences representing positive and negative sentiment\n",
    "positive_sentence = \"I am very happy with the results of this project.\"\n",
    "negative_sentence = \"I am disappointed with the outcome of this task.\"\n",
    "\n",
    "# Instantiating a simple DSPy module for sentiment classification\n",
    "dspy_sentiment_classification = dspy.Predict('sentence -> sentiment: bool')\n",
    "\n",
    "# Invoking the DSPy model with each respective sentence.\n",
    "print(f'Positive sentence: {dspy_sentiment_classification(sentence = positive_sentence)}')\n",
    "print(f'Negative sentence: {dspy_sentiment_classification(sentence = negative_sentence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "************ BUSINESS EMAIL ANALYSIS *************\n",
      "==================================================\n",
      "üìä SENTIMENT: Positive\n",
      "üìù TOPIC: Project changes and attached document\n",
      "üé© FORMALITY: Formal\n",
      "üî¢ WORD COUNT: 10\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "==================================================\n",
      "************ CASUAL MESSAGE ANALYSIS *************\n",
      "==================================================\n",
      "üìä SENTIMENT: Positive\n",
      "üìù TOPIC: Checking in and catching up\n",
      "üé© FORMALITY: Informal\n",
      "üî¢ WORD COUNT: 6\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Creating a class-based DSPy signature for text analysis\n",
    "class TextAnalysisSignature(dspy.Signature):\n",
    "    \"\"\"Analyze text for sentiment, main topic, and formality level.\"\"\"\n",
    "    \n",
    "    # Setting the input fields\n",
    "    text = dspy.InputField(desc = 'The text to be analyzed', default = '')\n",
    "    language = dspy.InputField(desc = 'The language of the text', default = 'English')\n",
    "    \n",
    "    # Setting the output fields\n",
    "    sentiment = dspy.OutputField(desc = 'The sentiment of the text (positive, negative, or neutral)', default = '')\n",
    "    topic = dspy.OutputField(desc = 'The main topic of the text', default = '')\n",
    "    formality = dspy.OutputField(desc = 'The formality level (formal, informal, or neutral)', default = '')\n",
    "    word_count = dspy.OutputField(type = int, desc = 'The number of words in the text', default = 0)\n",
    "\n",
    "# Creating a module using our custom signature\n",
    "dspy_text_analyzer = dspy.Predict(TextAnalysisSignature)\n",
    "\n",
    "# Creating sample texts for analysis\n",
    "business_email = \"Dear Ms. Smith, I wanted to inform you about the upcoming changes to our project timeline. Please review the attached document for details.\"\n",
    "casual_message = \"Hey there! Just wanted to check in and see how you're doing. Let's catch up soon!\"\n",
    "\n",
    "# Analyzing the texts\n",
    "business_analysis = dspy_text_analyzer(text = business_email, language = \"English\")\n",
    "casual_analysis = dspy_text_analyzer(text = casual_message, language = \"English\")\n",
    "\n",
    "# Displaying analysis results in a more creative way\n",
    "def display_analysis(title, analysis):\n",
    "    width = 50\n",
    "    print(\"=\" * width)\n",
    "    print(f\" {title} \".center(width, \"*\"))\n",
    "    print(\"=\" * width)\n",
    "    print(f\"üìä SENTIMENT: {analysis.sentiment}\")\n",
    "    print(f\"üìù TOPIC: {analysis.topic}\")\n",
    "    print(f\"üé© FORMALITY: {analysis.formality}\")\n",
    "    print(f\"üî¢ WORD COUNT: {analysis.word_count}\")\n",
    "    print(\"-\" * width)\n",
    "    \n",
    "display_analysis(\"BUSINESS EMAIL ANALYSIS\", business_analysis)\n",
    "print(\"\\n\")\n",
    "display_analysis(\"CASUAL MESSAGE ANALYSIS\", casual_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkhundley_venv",
   "language": "python",
   "name": "dkhundley_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
